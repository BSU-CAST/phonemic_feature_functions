{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "483082a1",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0123ad4a",
   "metadata": {},
   "source": [
    "#### Syllable Count (adjusts NLTK tokenizer for the *magic \"e\"* rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6494aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSP = SyllableTokenizer()\n",
    "def magic_e(word):\n",
    "    \n",
    "    result = SSP.tokenize(word)\n",
    "    syll_count = len(result)\n",
    "    \n",
    "    if syll_count == 1:\n",
    "        return syll_count\n",
    "    \n",
    "    if re.search('e$', result[len(result) - 1]):\n",
    "        modified = ''.join([result[i] for i in [len(result) - 2, len(result) - 1]])\n",
    "        result[len(result) - 2] = modified\n",
    "        del result[len(result) - 1]\n",
    "        syll_count = len(result)\n",
    "        \n",
    "    return syll_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c12644",
   "metadata": {},
   "source": [
    "#### POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5d1c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_POS(row):\n",
    "    \n",
    "    retList = []\n",
    "    \n",
    "    for tag in nltk.pos_tag(row):\n",
    "        retList.append(tag[1])\n",
    "    \n",
    "    return retList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e4c6b3",
   "metadata": {},
   "source": [
    "#### Get IPA translations (using provided translations from https://github.com/open-dict-data/ipa-dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a1e5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('phoneticDictionary.csv')\n",
    "df = pd.DataFrame(list(zip(df['word'], df['phon'])), columns=['word', 'ipa'])\n",
    "\n",
    "def ipa(section):\n",
    "    total_words = 0\n",
    "    ipa_word = list(df['word'])\n",
    "    ipa_notation = list(df['ipa'])\n",
    "    ipa_dict = dict(zip(ipa_word, ipa_notation))\n",
    "    new_sent = []\n",
    "    for row in section.text:\n",
    "        sent = []\n",
    "        words = 0\n",
    "        for word in row:\n",
    "            total_words += 1\n",
    "            words += 1\n",
    "            if word in ipa_dict.keys():\n",
    "                this_word = ipa_dict[word].replace(\"ˈ\", \"\")\n",
    "                this_word = this_word.replace(\"ˌ\", \"\")\n",
    "                sent.append(this_word)\n",
    "            elif word in punctuation:\n",
    "                sent.append(word)\n",
    "            else:\n",
    "                sent.append(' ')\n",
    "        new_sent.append(sent)\n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eeafa2",
   "metadata": {},
   "source": [
    "#### Functions to retrieve Suffix Tree Language Model for Greek/Latin roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9194aa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_STLM(train_text):\n",
    "    trie = SuffixTree()\n",
    "    sents = []\n",
    "    words = []\n",
    "    for sent in train_text:\n",
    "        sents.append(sent)\n",
    "        for word in sent:\n",
    "            words.append(word)\n",
    "            trie.add(word)\n",
    "        \n",
    "    trie.update_all_counts()\n",
    "    stlm = STLM(trie)\n",
    "    \n",
    "    return stlm\n",
    "\n",
    "def get_STLM_prob(stlm, test):\n",
    "    y = 0\n",
    "    seq = Sequence()\n",
    "    for t in test: seq.push_back(t)\n",
    "        \n",
    "    return stlm.prob(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9cd32b",
   "metadata": {},
   "source": [
    "## Letter-Name Alphabetic Stage Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b034811e",
   "metadata": {},
   "source": [
    "#### Check for words with consonant-vowel-consonant short vowel pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691d6382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_CVC_short(dataset):\n",
    "    \n",
    "    CVC_short = []\n",
    "    for row in tqdm(dataset['ipa']):\n",
    "        cvc = 0\n",
    "        total_words = 0\n",
    "        for word in row:\n",
    "            total_words += 1\n",
    "            if re.search('^[btkzɹsjmfgndɫwpθvhʃð][btkzɹsjmfgndɫwpθvhʃðʒŋ]*[ɪɑæəʊɛ][btkzɹsjmfgndɫwpθvhʃðʒŋ]*[btkzɹsjmfgndɫwpθvhʃðʒŋ]$', word):\n",
    "                cvc += 1\n",
    "        \n",
    "        CVC_short.append(cvc / total_words)\n",
    "    \n",
    "    return CVC_short"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97afd08",
   "metadata": {},
   "source": [
    "## Within-Word Pattern Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec6fcf5",
   "metadata": {},
   "source": [
    "#### Check for basic inflectionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06b4d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_basic_inflectional(dataset):\n",
    "    \n",
    "    verb_tags = ['VBD', 'VBG', 'VBN', 'VBZ']\n",
    "    text_POS = list(zip(dataset['text'], dataset['POS']))\n",
    "    inflectional = []\n",
    "    \n",
    "    for item in tqdm(text_POS):\n",
    "        total_words = 0\n",
    "        inf_end = 0\n",
    "        i = 0\n",
    "        for word in item[0]:\n",
    "            total_words += 1\n",
    "            if item[1][i] in verb_tags:\n",
    "                if re.search('es$', word):\n",
    "                    inf_end += 1\n",
    "                if re.search('s$', word):\n",
    "                    inf_end += 1\n",
    "            i += 1\n",
    "            \n",
    "        inflectional.append(inf_end / total_words)\n",
    "    \n",
    "    return inflectional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa6fcb4",
   "metadata": {},
   "source": [
    "#### Check for complex consonants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d94f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_complex_cons(dataset):\n",
    "    \n",
    "    _complex = []\n",
    "    text_ipa = list(zip(dataset['text'], dataset['ipa']))\n",
    "    \n",
    "    for item in tqdm(text_ipa):\n",
    "        total_words = 0\n",
    "        com_cons = 0\n",
    "        i = 0\n",
    "        for word in item[0]:\n",
    "            total_words += 1\n",
    "            sylls = magic_e(word)\n",
    "            if sylls == 1:\n",
    "                if re.search('g', item[0][i]):\n",
    "                    if not re.search('g', item[1][i]):\n",
    "                        com_cons += 1\n",
    "                if re.search('^w', item[0][i]):\n",
    "                    if not re.search('w', item[1][i]):\n",
    "                        com_cons += 1\n",
    "                if re.search('c', item[0][i]):\n",
    "                    if not re.search('k', item[1][i]):\n",
    "                        com_cons += 1\n",
    "                if re.search('k', item[0][i]):\n",
    "                    if not re.search('k', item[1][i]):\n",
    "                        com_cons += 1\n",
    "                if re.search('dʒ$', item[1][i]):\n",
    "                    com_cons += 1\n",
    "                if re.search('se$', item[0][i]):\n",
    "                    if re.search('z$', item[1][i]):\n",
    "                        com_cons += 1\n",
    "                if re.search('b$', item[0][i]):\n",
    "                    if not re.search('b', item[1][i]):\n",
    "                        com_cons += 1\n",
    "                if re.search('ce$', item[0][i]):\n",
    "                    if re.search('s$', item[1][i]):\n",
    "                        com_cons += 1\n",
    "                if re.search('[btkzrsjmfndlwpvhg]ch$', item[0][i]):\n",
    "                    com_cons += 1\n",
    "                if re.search('[btkzrsjmfndlwpvhg]ge$', item[0][i]):\n",
    "                    com_cons += 1\n",
    "                    \n",
    "            i += 1\n",
    "            \n",
    "        _complex.append(com_cons / total_words)\n",
    "        \n",
    "    return _complex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e610a8a",
   "metadata": {},
   "source": [
    "## Syllables & Affixes Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306e47db",
   "metadata": {},
   "source": [
    "#### Check type of syllable juncture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97b9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VV(dataset):\n",
    "    \n",
    "    DIPHTHONGS = ['aɪ', 'eɪ', 'ɪə', 'ɔɪ', 'aʊ', 'oʊ', 'ʊə', 'eə']\n",
    "    \n",
    "    all_vv = []\n",
    "    for row in tqdm(dataset['ipa']):\n",
    "        total_words = 0\n",
    "        vv = 0\n",
    "        for word in row:\n",
    "            total_words += 1\n",
    "            if re.search('[ɪɑæəʊɛiuɔaoe][ɪɑæəʊɛiuɔaoe][ɪɑæəʊɛiuɔaoe]', word):\n",
    "                vv += 1\n",
    "            elif re.search('[ɪɑæəʊɛiuɔaoe][ɫɝ]', word):\n",
    "                vv += 1\n",
    "            elif re.search('[ɪɑæəʊɛiuɔaoe][ɪɑæəʊɛiuɔaoe]', word):\n",
    "                result = re.findall('[ɪɑæəʊɛiuɔaoe][ɪɑæəʊɛiuɔaoe]', word)\n",
    "                for res in result:\n",
    "                    if res not in DIPHTHONGS:\n",
    "                        vv += 1\n",
    "        all_vv.append(vv / total_words)\n",
    "        \n",
    "    return all_vv\n",
    "    \n",
    "    \n",
    "def VCCV_doublet(dataset):\n",
    "    \n",
    "    vccv_doublet = []\n",
    "    for row in tqdm(dataset['text']):\n",
    "        total_words = 0\n",
    "        vccv = 0\n",
    "        for word in row:\n",
    "            total_words += 1\n",
    "            sylls = magic_e(word)\n",
    "            if sylls < 2:\n",
    "                break\n",
    "            if re.search('[aeiou][btkzrsjmfndlwpvhg][btkzrsjmfndlwpvhg][aeiou]', word):\n",
    "                result = magic_e_result(word)\n",
    "                if result[0][-1] in CONSONANTS_TEXT:\n",
    "                    con = result[0][-1]\n",
    "                    if con == result[1][0]:\n",
    "                        vccv += 1\n",
    "                \n",
    "        vccv_doublet.append(vccv / total_words)\n",
    "        \n",
    "    return vccv_doublet\n",
    "    \n",
    "\n",
    "def VCCV(dataset):\n",
    "    \n",
    "    vccv_all = []\n",
    "    \n",
    "    for row in tqdm(dataset['text']):\n",
    "        total_words = 0\n",
    "        vccv = 0\n",
    "        for word in row:\n",
    "            total_words += 1\n",
    "            sylls = magic_e(word)\n",
    "            if sylls < 2:\n",
    "                break\n",
    "            if re.search('[aeiou][btkzrsjmfndlwpvhg][btkzrsjmfndlwpvhg][aeiou]', word):\n",
    "                result = magic_e_result(word)\n",
    "                if result[0][-1] in CONSONANTS_TEXT:\n",
    "                    con = result[0][-1]\n",
    "                    if con != result[1][0]:\n",
    "                        vccv += 1\n",
    "                    \n",
    "        vccv_all.append(vccv / total_words)\n",
    "            \n",
    "    return vccv_all\n",
    "\n",
    "\n",
    "def VCCCV(dataset):\n",
    "    \n",
    "    vcccv_all = []\n",
    "    \n",
    "    for row in tqdm(dataset['text']):\n",
    "        total_words = 0\n",
    "        vcccv = 0\n",
    "        for word in row:\n",
    "            total_words += 1\n",
    "            sylls = magic_e(word)\n",
    "            if sylls == 2:\n",
    "                if re.search('[aeiou][btkzrsjmfndlwpvhg][btkzrsjmfndlwpvhg][btkzrsjmfndlwpvhg][aeiou]', word):\n",
    "                    vcccv += 1\n",
    "                    \n",
    "        vcccv_all.append(vcccv / total_words)\n",
    "            \n",
    "    return vcccv_all\n",
    "\n",
    "\n",
    "def VVCV(dataset):\n",
    "    \n",
    "    vvcv_all = []\n",
    "    \n",
    "    for row in tqdm(dataset['text']):\n",
    "        total_words = 0\n",
    "        vvcv = 0\n",
    "        for word in row:\n",
    "            total_words += 1\n",
    "            sylls = magic_e(word)\n",
    "            if sylls == 2:\n",
    "                if re.search('[aeiou][aeiou][btkzrsjmfndlwpvhg][aeiou]', word):\n",
    "                    vvcv += 1\n",
    "                    \n",
    "        vvcv_all.append(vvcv / total_words)\n",
    "            \n",
    "    return vvcv_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea00e81",
   "metadata": {},
   "source": [
    "#### Check for compound words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7cf55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_compound_words_greedy(dataset):\n",
    "\n",
    "    cmp_words = []\n",
    "    words = list(df['word'])\n",
    "    \n",
    "    for row in tqdm(dataset['text']):\n",
    "        compound = 0\n",
    "        total_words = 0\n",
    "        i = 0\n",
    "        for word in row:\n",
    "            total_words += 1\n",
    "            if magic_e(word) > 1:\n",
    "                if len(word) > 4:\n",
    "                    if word[:2] in words:\n",
    "                        if word[2:] in words:\n",
    "                            compound += 1\n",
    "                    if word[:3] in words:\n",
    "                        if word[3:] in words:\n",
    "                            compound += 1\n",
    "                    if word[:4] in words:\n",
    "                        if word[4:] in words:\n",
    "                            compound += 1\n",
    "                if len(word) > 5:\n",
    "                    if word[:5] in words:\n",
    "                        if word[5:] in words:\n",
    "                            compound += 1\n",
    "                if len(word) > 6:\n",
    "                    if word[:6] in words:\n",
    "                        if word[6:] in words:\n",
    "                            compound += 1\n",
    "            elif word in COMPOUND_WORDS:\n",
    "                compound += 1\n",
    "            i += 1\n",
    "                \n",
    "        cmp_words.append(compound / total_words)           \n",
    "    \n",
    "    return cmp_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015b7bab",
   "metadata": {},
   "source": [
    "#### Check for advanced inflectional endings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94fec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_adv_inflectional(dataset):\n",
    "    \n",
    "    verb_tags = ['VBD', 'VBG', 'VBN', 'VBZ']\n",
    "    text_POS = list(zip(dataset['text'], dataset['POS']))\n",
    "    inflectional = []\n",
    "    \n",
    "    for item in tqdm(text_POS):\n",
    "        total_words = 0\n",
    "        inf_end = 0\n",
    "        i = 0\n",
    "        for word in item[0]:\n",
    "            total_words += 1\n",
    "            if item[1][i] in verb_tags:\n",
    "                if re.search('ing$', word):\n",
    "                    inf_end += 1\n",
    "                if re.search('ed$', word):\n",
    "                    inf_end += 1\n",
    "            i += 1\n",
    "            \n",
    "        inflectional.append(inf_end / total_words)\n",
    "    \n",
    "    return inflectional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861fb510",
   "metadata": {},
   "source": [
    "#### Check for inflectional endings for adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7284e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_adv_inflectional_adj(dataset):\n",
    "    \n",
    "    adj_tags = ['JJR', 'JJS', 'RBR', 'RBS', 'JJ']\n",
    "    text_POS = list(zip(dataset['text'], dataset['POS']))\n",
    "    inflectional = []              \n",
    "    \n",
    "    for item in tqdm(text_POS):\n",
    "        total_words = 0\n",
    "        inf_end = 0\n",
    "        i = 0\n",
    "        for word in item[0]:\n",
    "            total_words += 1\n",
    "            if item[1][i] in adj_tags:\n",
    "                if item[1][i] == 'JJ':\n",
    "                    if re.search('ful$', word):\n",
    "                        inf_end += 1\n",
    "                    if re.search('ness$', word):\n",
    "                        inf_end += 1\n",
    "                    if re.search('less$', word):\n",
    "                        inf_end += 1\n",
    "                    if re.search('ily$', word):\n",
    "                        inf_end += 1\n",
    "                else:\n",
    "                    inf_end += 1\n",
    "                    \n",
    "            i += 1\n",
    "            \n",
    "        inflectional.append(inf_end / total_words)\n",
    "    \n",
    "    return inflectional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c27867",
   "metadata": {},
   "source": [
    "## Derivational Relations Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d384edf2",
   "metadata": {},
   "source": [
    "#### Check for advanced suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23175531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_adv_suffix(dataset):\n",
    "    \n",
    "    adjs_nouns = ['JJR', 'JJS', 'JJ', 'NN', 'NNP', 'NNS']\n",
    "    verbs = ['VBD', 'VBG', 'VBN', 'VBZ']\n",
    "    adj_n_suffix = ['ɛɹi$', 'ɔɹi$', 'ənsi$', 'əns$', 'ʒən', 'ʃən', 'əbəɫ$', 'əbɫi$']\n",
    "    v_suffix = ['aɪz', 'ɪfaɪ', 'əfaɪ']\n",
    "    adv_suf = []\n",
    "    ipa_text = list(zip(dataset['ipa'], dataset['text'], dataset['POS']))\n",
    "    for item in tqdm(ipa_text):\n",
    "        total_words = 0\n",
    "        adv = 0\n",
    "        i = 0\n",
    "        for word in item[0]:\n",
    "            if len(word) > 0:\n",
    "                total_words += 1\n",
    "                if magic_e(item[1][i]) > 1:\n",
    "                    if item[2][i] in adjs_nouns:\n",
    "                        for suf in adj_n_suffix:\n",
    "                            if re.search(suf, word):\n",
    "                                adv += 1\n",
    "                    if item[2][i] in verbs:\n",
    "                        for suf in v_suffix:\n",
    "                            if re.search(suf, word):\n",
    "                                adv += 1\n",
    "                                \n",
    "            i += 1\n",
    "        adv_suf.append(adv / total_words)\n",
    "        \n",
    "    return adv_suf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7eff7e",
   "metadata": {},
   "source": [
    "#### Check for assimilated prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715e189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_assimilated(dataset):\n",
    "    assim_prefix = []\n",
    "    for row in tqdm(dataset['text']):\n",
    "        assimilated = 0\n",
    "        total_words = 0\n",
    "        for word in row:\n",
    "            total_words += 1\n",
    "            if len(magic_e_result(word)) > 1:\n",
    "                if re.search('^ill', word):\n",
    "                    assimilated += 1\n",
    "                if re.search('^imm[aeiou]', word):\n",
    "                    assimilated += 1\n",
    "                if re.search('^imp', word):\n",
    "                    assimilated += 1\n",
    "                if re.search('^irr[aeiou]', word):\n",
    "                    assimilated += 1\n",
    "                if re.search('^suff', word):\n",
    "                    assimilated += 1\n",
    "                if re.search('^supp', word):\n",
    "                    assimilated += 1\n",
    "                if re.search('^succ', word):\n",
    "                    assimilated += 1\n",
    "                if re.search('^surr', word):\n",
    "                    assimilated += 1\n",
    "                if re.search('^coll', word):\n",
    "                    assimilated += 1\n",
    "                if re.search('^corr', word):\n",
    "                    assimilated += 1\n",
    "                if re.search('^att', word):\n",
    "                    assimilated += 1\n",
    "                if re.search('^aff', word):\n",
    "                    assimilated += 1\n",
    "                if re.search('^agg', word):\n",
    "                    assimilated += 1\n",
    "                if re.search('^all', word):\n",
    "                    assimilated += 1\n",
    "                if re.search('^ann', word):\n",
    "                    assimilated += 1\n",
    "                if re.search('^app', word):\n",
    "                    if not re.search('apples', word):\n",
    "                        assimilated += 1\n",
    "                if re.search('^ass', word):\n",
    "                    assimilated += 1\n",
    "                if re.search('^arr', word):\n",
    "                    assimilated += 1\n",
    "                if re.search('^diff', word):\n",
    "                    assimilated += 1\n",
    "                if re.search('^eff', word):\n",
    "                    assimilated += 1\n",
    "                if re.search('^opp', word):\n",
    "                    assimilated += 1\n",
    "                if re.search('^off', word):\n",
    "                    assimilated += 1\n",
    "                if re.search('^occ', word):\n",
    "                    assimilated += 1\n",
    "                    \n",
    "        assim_prefix.append(assimilated / total_words)\n",
    "                    \n",
    "    return assim_prefix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4403e7",
   "metadata": {},
   "source": [
    "#### Check for Greek roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a4bc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_greek_roots(row):\n",
    "    \n",
    "    greek_list = GREEK_ROOTS.keys()\n",
    "\n",
    "    total_prob = 0\n",
    "    total_words = 0\n",
    "    \n",
    "    for word in row:\n",
    "        total_words += 1\n",
    "        prob = 0\n",
    "        if magic_e(word) > 1:\n",
    "            for root in greek_list:\n",
    "                if re.search(root, word):\n",
    "                    prob = get_STLM_prob(greek_stlms[root], word)\n",
    "        total_prob += prob\n",
    "    \n",
    "    return total_prob / total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e042776f",
   "metadata": {},
   "source": [
    "#### Check for Latin roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d1ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_latin_roots(row):\n",
    "    \n",
    "    latin_list = LATIN_ROOTS.keys()\n",
    "    \n",
    "    total_prob = 0\n",
    "    total_words = 0\n",
    "    \n",
    "    for word in row:\n",
    "        prob = 0\n",
    "        total_words += 1\n",
    "        if word.isalpha():\n",
    "            if magic_e(word) > 1:\n",
    "                for root in latin_list:\n",
    "                    if re.search(root, word):\n",
    "                        prob = get_STLM_prob(latin_stlms[root], word)\n",
    "            total_prob += prob\n",
    "    \n",
    "    return total_prob / total_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
